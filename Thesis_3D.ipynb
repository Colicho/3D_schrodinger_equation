{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import torch.optim.lr_scheduler as lr_scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = 1 # Number of dimensions\n",
    "Nh = 100 # Hidden dimension \n",
    "num_layers = 2\n",
    "num_samples = 50\n",
    "\n",
    "l = 5\n",
    "data_range = (-l, l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Laplacian calculation and different potentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kinematics_f(x, psi):\n",
    "    constant = -1/2\n",
    "    grad_x_values = torch.autograd.grad(psi.sum(), x, create_graph=True)[0]\n",
    "    laplacian_values = torch.zeros_like(x) \n",
    "    \n",
    "    # Compute the Laplacian for each dimension separately\n",
    "    for i in range(x.shape[1]):\n",
    "        laplacian_dim = torch.autograd.grad(grad_x_values[:, i].sum(), x, create_graph=True)[0]    \n",
    "        laplacian_values[:, i] = laplacian_dim[:, i]\n",
    "    laplacian_values = torch.sum(laplacian_values, dim=1).view(-1,1)\n",
    "    return constant * laplacian_values, grad_x_values\n",
    "\n",
    "def HO_potential(x):\n",
    "    potential = torch.square(x)/2\n",
    "    potential_3d = torch.sum(potential, dim=1).view(-1,1)\n",
    "    return potential_3d\n",
    "\n",
    "def Nilsson_model(x):\n",
    "    x_freq = 4\n",
    "    y_freq = 2\n",
    "    z_freq = 2\n",
    "    if x.shape[1] == 2: \n",
    "        freq_reg = torch.tensor([x_freq, y_freq])\n",
    "    elif x.shape[1] == 3: \n",
    "        freq_reg = torch.tensor([x_freq, y_freq, z_freq])\n",
    "    potential = freq_reg * torch.square(x)/2\n",
    "    potential_3d = torch.sum(potential, dim=1).view(-1,1)\n",
    "    return potential_3d\n",
    "\n",
    "def HA_potential(x):\n",
    "    r = torch.sqrt(torch.sum(torch.square(x), dim=1))\n",
    "    potential_3d = -1 / r\n",
    "    return potential_3d.view(-1,1)\n",
    "\n",
    "def Woods_potential(x):\n",
    "    V_0 = 6.5\n",
    "    a = 0.3\n",
    "    R = 3\n",
    "    r = torch.sqrt(torch.sum(torch.square(x), dim=1))\n",
    "    potential_3d = -V_0 / (1 + torch.exp((r - R) / a))\n",
    "    return potential_3d.view(-1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A loss function which takes into account the different elements of a wave function.\n",
    "1. The main differential equation denoted as L_DE.\n",
    "2. The normalization criterion for a wave function denoted as L_norm.\n",
    "3. The orthogonality between different solutions denoted as L_orth.\n",
    "4. The boundary conditions denoted as L_BC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def schrodinger_loss(x, psi, E, data_range, solutions, model_list, edge_indices):\n",
    "    M = x.shape[0]\n",
    "    x_R = data_range[1]\n",
    "    x_L = data_range[0]\n",
    "    L_norm = torch.square(torch.sum(torch.square(psi)) * ((x_R - x_L)**x.shape[1])/M - 1)\n",
    "\n",
    "    psi_eigen = torch.zeros((M, 1))\n",
    "    for i in range(solutions):\n",
    "        psi_eigen += torch.square(torch.mean(psi * model_list[i](x)[0]))\n",
    "    L_orth = torch.sum(psi_eigen)\n",
    "    \n",
    "    kinematics, grads = kinematics_f(x, psi)\n",
    "    potential = HO_potential(x)\n",
    "    H_psi = kinematics + potential * psi\n",
    "    L_DE = torch.mean(torch.square(H_psi - E * psi))\n",
    "\n",
    "    L_BC = torch.sum(torch.square(psi[edge_indices]) + torch.square(grads[edge_indices]))\n",
    "    \n",
    "    total_loss = L_DE + L_norm + L_orth + L_BC\n",
    "    return total_loss, L_DE, L_orth, L_norm, L_BC, torch.sum(psi * H_psi) / torch.sum(psi * psi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NN(D, Nh, num_layers)\n",
    "\n",
    "learning_rates = {'lambda_layer': 0.001, 'input_layer': 0.00005, 'output_layer': 0.00005}\n",
    "for i in range(num_layers):\n",
    "    layer_name = f\"h_layer{i+1}\"\n",
    "    learning_rates[layer_name] = 0.0005\n",
    "parameters = [{'params': getattr(model, name).parameters(), 'lr': lr} for name, lr in learning_rates.items()]\n",
    "\n",
    "optimizer = optim.Adam(parameters, lr=0.01)\n",
    "step_size = 2000\n",
    "gamma = 0.95\n",
    "scheduler = lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)\n",
    "\n",
    "loss_history, E_history, model_list, l_model = train1D(model, optimizer, scheduler, learning_rates, 10000, num_samples, data_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MODEL\n",
    "\n",
    "class NN(nn.Module):\n",
    "    def __init__(self, D, Nh, num_layers):\n",
    "        super(NN, self).__init__()\n",
    "        self.lambda_layer = nn.Linear(1, 1)\n",
    "        self.input_layer = nn.Linear(1 + D, Nh)\n",
    "        self.num_layers = num_layers\n",
    "        for i in range(num_layers):\n",
    "            setattr(self, f\"h_layer{i+1}\", nn.Linear(Nh + 1, Nh))\n",
    "        self.layers = nn.ModuleList([nn.Linear(D + 1, Nh)])\n",
    "        self.output_layer = nn.Linear(Nh + 1, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        I = torch.ones((x.shape[0], 1), requires_grad=True)\n",
    "        E = self.lambda_layer(I)\n",
    "\n",
    "        input = torch.cat([x, E], dim = 1)\n",
    "        h = torch.sin(self.input_layer(input))\n",
    "        for i in range(self.num_layers):\n",
    "            input_h = torch.cat([h, E], dim = 1)\n",
    "            layer = getattr(self, f\"h_layer{i+1}\")\n",
    "            h = torch.sin(layer(input_h))\n",
    "        input_h = torch.cat([h, E], dim = 1)\n",
    "\n",
    "        pred = self.output_layer(input_h)\n",
    "        return pred, E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(np.array(E_history)[4000:, 0])\n",
    "plt.plot(np.array(E_history)[4000:, 1])\n",
    "plt.legend((\"Model Energy\", \"Variational Energy\"))\n",
    "plt.title(\"Energy Evolution (1D)\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Energy\")\n",
    "num_points = 6000\n",
    "\n",
    "x_ticks = np.linspace(0, num_points, num=7, dtype=int)  # 6 ticks evenly spaced\n",
    "x_labels = np.linspace(4000, 4000 + num_points, num=7, dtype=int)\n",
    "plt.xticks(ticks=x_ticks, labels=x_labels)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train1D(model, optimizer, scheduler, learning_rates, epochs, num_samples, data_range, model_list = []):\n",
    "    loss_history = []\n",
    "    E_history = []\n",
    "    solutions = 0\n",
    "    convergence = 10000\n",
    "    \n",
    "    data = torch.linspace(data_range[0], data_range[1], num_samples).view(-1,1)\n",
    "    data.requires_grad = True\n",
    "\n",
    "    edge_indices = torch.any((data == data_range[0]) | (data == data_range[1]), dim=1).nonzero(as_tuple=False).squeeze()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        if epoch != 0 and epoch % convergence == 0:\n",
    "            model_list.append(copy.deepcopy(model))\n",
    "            solutions += 1\n",
    "            for param_group, (_, lr) in zip(optimizer.param_groups, learning_rates.items()):\n",
    "                param_group['lr'] = lr\n",
    "\n",
    "        pred, E = model(data)\n",
    "        loss, L_DE, L_orth, L_norm, L_BC, var_E= schrodinger_loss(data, pred, E, data_range, solutions, model_list, edge_indices)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        loss_history.append(L_DE)\n",
    "        E_history.append((E.mean().item(), var_E.item()))\n",
    "        \n",
    "        if epoch % 500 == 0:\n",
    "            x = torch.linspace(data_range[0], data_range[1], 2000)\n",
    "            plt.figure(figsize=(8, 4))\n",
    "            plt.plot(x, model(x.view(-1,1))[0].detach().numpy())\n",
    "            plt.show()\n",
    "            print(f'Epoch [{epoch + 1}/{epochs}], Loss: {loss}, L_orth: {L_orth}, Energy: {E[0].item()}, LDE_Loss: {L_DE.item():.4f}, Vari_E: {var_E.item():.4f}, Solutions: {solutions}')\n",
    "    return loss_history, E_history, model_list, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train2D(model, optimizer, scheduler, learning_rates, epochs, num_samples, data_range, model_list = []):\n",
    "    loss_history = []\n",
    "    E_history = []\n",
    "    solutions = len(model_list)\n",
    "    threshold = 2e-5\n",
    "\n",
    "    data1 = torch.linspace(data_range[0], data_range[1], num_samples)\n",
    "    data2 = torch.linspace(data_range[0], data_range[1], num_samples)\n",
    "    X, Y = torch.meshgrid(data1, data2)\n",
    "\n",
    "    data1 = X.reshape(-1, 1)\n",
    "    data2 = Y.reshape(-1, 1)\n",
    "    data_concat = torch.hstack((data1, data2))\n",
    "    data_concat.requires_grad = True\n",
    "\n",
    "    edge_indices = torch.any((data_concat == data_range[0]) | (data_concat == data_range[1]), dim=1).nonzero(as_tuple=False).squeeze()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        pred, E  = model(data_concat)     \n",
    "        loss, L_DE, L_orth, L_norm, L_BC, var_E = schrodinger_loss(data_concat, pred, E, data_range, solutions, model_list, edge_indices)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        loss_history.append(loss.item())\n",
    "        \n",
    "        E_history.append((E.mean().item(), var_E.item()))\n",
    "\n",
    "        if loss < threshold:\n",
    "            model_list.append(copy.deepcopy(model))\n",
    "            solutions += 1\n",
    "            for param_group, (_, lr) in zip(optimizer.param_groups, learning_rates.items()):\n",
    "                param_group['lr'] = lr\n",
    "        if solutions == 4:\n",
    "            break\n",
    "        \n",
    "        if epoch % 100 == 0:\n",
    "            psi_2d = model(data_concat)[0].detach().numpy().reshape(num_samples, num_samples)\n",
    "            plt.figure(figsize=(8, 6))\n",
    "            plt.contourf(X, Y, psi_2d, cmap='viridis')\n",
    "            plt.colorbar(label='Probability amplitude')\n",
    "            plt.show()\n",
    "            print(f'Epoch [{epoch + 1}/{epochs}], Loss: {loss}, LDE_Loss: {L_DE.item():.4f}, Orth_Loss: {L_orth.item():.4f}, Eigenvalue: {E[0].item():.4f}, Vari_E: {var_E.item():.4f}, Solutions: {solutions}')\n",
    "    return loss_history, E_history, model_list, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train3D(model, optimizer, scheduler, learning_rates, epochs, num_samples, data_range, model_params = None):\n",
    "    loss_history = []\n",
    "    E_history = []\n",
    "    solutions = len(model_list)\n",
    "    convergence = 10000\n",
    "\n",
    "    data1 = torch.linspace(data_range[0], data_range[1], num_samples)\n",
    "    data2 = torch.linspace(data_range[0], data_range[1], num_samples)\n",
    "    data3 = torch.linspace(data_range[0], data_range[1], num_samples)\n",
    "    X, Y, Z = torch.meshgrid(data1, data2, data3)\n",
    "    data1 = X.reshape(-1, 1)\n",
    "    data2 = Y.reshape(-1, 1)\n",
    "    data3 = Z.reshape(-1, 1)\n",
    "    data_concat = torch.hstack((data1, data2, data3))\n",
    "    data_concat.requires_grad = True\n",
    "\n",
    "    edge_indices = torch.any((data_concat == data_range[0]) | (data_concat == data_range[1]), dim=1).nonzero(as_tuple=False).squeeze()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        if epoch != 0 and epoch % convergence == 0:\n",
    "            model_list.append(copy.deepcopy(model))\n",
    "            solutions += 1\n",
    "            for param_group, (_, lr) in zip(optimizer.param_groups, learning_rates.items()):\n",
    "                param_group['lr'] = lr\n",
    "\n",
    "        pred, E = model(data_concat)\n",
    "        loss, L_DE, L_orth, L_reg, L_BC, var_E = schrodinger_loss(data_concat, pred, E, data_range, solutions, model_list, edge_indices)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        loss_history.append(loss.item())\n",
    "        E_history.append((E.mean().item(), var_E.item()))\n",
    "   \n",
    "        if epoch % 50 == 0:\n",
    "            psi_3d = model(data_concat)[0].detach().numpy().reshape(num_samples, num_samples, num_samples)\n",
    "            probability_density_flat = psi_3d.flatten()\n",
    "\n",
    "            opacity = np.where(np.abs(probability_density_flat) < 0.2, 0.05, 0.8)\n",
    "\n",
    "            fig = plt.figure()\n",
    "            ax = fig.add_subplot(111, projection='3d')\n",
    "            ax.set_xlabel('X')\n",
    "            ax.set_ylabel('Y')\n",
    "            ax.set_zlabel('Z')\n",
    "            scatter = ax.scatter(data1, data2, data3, c=probability_density_flat, cmap='viridis', alpha = opacity)\n",
    "            cbar = fig.colorbar(scatter)\n",
    "            cbar.set_label('Probability Density')\n",
    "            plt.show()\n",
    "\n",
    "            # Projection onto 2D contour plot\n",
    "            fig2 = plt.figure()\n",
    "            ax2 = fig2.add_subplot(111)\n",
    "            contour = ax2.contourf(X[:, :, 0], Y[:, :, 0], psi_3d.sum(axis=2), cmap='viridis')\n",
    "            cbar2 = fig2.colorbar(contour)\n",
    "            cbar2.set_label('Probability Density')\n",
    "\n",
    "            plt.show()\n",
    "            \n",
    "            print(f'Epoch [{epoch + 1}/{epochs}], Loss: {loss}, LDE_Loss: {L_DE.item():.4f}, Orth_Loss: {L_orth.item():.4f}, Norm_Loss: {L_reg.item():.4f}, BC_Loss: {L_BC.item():.4f}, Eigenvalue: {E[0].item():.4f}, Vari_E: {var_E.item():.4f}, Solutions: {solutions}')\n",
    "    return loss_history, E_history, model_list, model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Below lies code for testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "num = 5000\n",
    "min_val = -5\n",
    "max_val = 5\n",
    "x = (max_val - min_val) * torch.rand(num) + min_val\n",
    "y = (max_val - min_val) * torch.rand(num) + min_val\n",
    "\n",
    "concatenated_data = torch.stack((x, y), dim=1)\n",
    "z = model_list[1](concatenated_data)[0].flatten().detach().numpy()\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(projection='3d')\n",
    "ax.plot_trisurf(x, y, z, cmap='coolwarm')\n",
    "\n",
    "num_ticks = 3\n",
    "ax.tick_params(axis='x', labelsize=20) \n",
    "ax.tick_params(axis='y', labelsize=20)\n",
    "ax.tick_params(axis='z', labelsize=20)\n",
    "x_ticks = [min_val, 0, max_val]\n",
    "y_ticks = [min_val, 0, max_val]\n",
    "ax.set_xticks(x_ticks)\n",
    "ax.set_yticks(y_ticks)\n",
    "ax.set_zticks([-0.7, 0, 0.7])\n",
    "print(np.max(z))\n",
    "print(np.min(z))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num = 50\n",
    "min_val = -5\n",
    "max_val = 5\n",
    "\n",
    "data1 = torch.linspace(min_val, max_val, num)\n",
    "data2 = torch.linspace(min_val, max_val, num)\n",
    "X, Y = torch.meshgrid(data1, data2)\n",
    "\n",
    "data1 = X.reshape(-1, 1)\n",
    "data2 = Y.reshape(-1, 1)\n",
    "data_concat = torch.hstack((data1, data2))\n",
    "data_concat.requires_grad = True\n",
    "pred, E = model_list[2](data_concat)\n",
    "\n",
    "kinematics, grads = kinematics_f(data_concat, pred)\n",
    "H_psi = kinematics + Nilsson_model(data_concat) * pred\n",
    "\n",
    "print(E.mean(), torch.sum(pred * H_psi) / torch.sum(pred * pred))\n",
    "\n",
    "psi_2d = pred.detach().numpy().reshape(num, num)\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.contourf(X, Y, psi_2d, cmap='coolwarm')\n",
    "colorbar = plt.colorbar(ticks=[-0.7, 0, 0.7])\n",
    "colorbar.ax.tick_params(labelsize=30)  # Set font size for color bar labels\n",
    "plt.yticks([-5, 0, 5], fontsize=30)\n",
    "plt.xticks([-5, 0, 5], fontsize=30)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3D Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num = 25\n",
    "min_val = -5\n",
    "max_val = 5\n",
    "data1 = torch.linspace(min_val, max_val, num)\n",
    "data2 = torch.linspace(min_val, max_val, num)\n",
    "data3 = torch.linspace(min_val, max_val, num)\n",
    "X, Y, Z = torch.meshgrid(data1, data2, data3)\n",
    "data1 = X.reshape(-1, 1)\n",
    "data2 = Y.reshape(-1, 1)\n",
    "data3 = Z.reshape(-1, 1)\n",
    "data_concat = torch.hstack((data1, data2, data3))\n",
    "data_concat.requires_grad = True\n",
    "\n",
    "pred, E = l_model(data_concat)\n",
    "\n",
    "kinematics, grads = kinematics_f(data_concat, pred)\n",
    "H_psi = kinematics + HO_potential(data_concat) * pred\n",
    "\n",
    "print(E.mean(), torch.sum(pred * H_psi) / torch.sum(pred * pred))\n",
    "\n",
    "psi_3d = pred.detach().numpy().reshape(num, num, num)\n",
    "probability_density_flat = psi_3d.flatten()\n",
    "print(np.max(probability_density_flat))\n",
    "print(np.min(probability_density_flat))\n",
    "\n",
    "opacity = np.where(np.abs(probability_density_flat) < 0.05, 0.01, 0.8)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "num_ticks = 3\n",
    "ax.tick_params(axis='x', labelsize=20) \n",
    "ax.tick_params(axis='y', labelsize=20)\n",
    "ax.tick_params(axis='z', labelsize=20)\n",
    "ticks = [min_val, 0, max_val]\n",
    "ax.set_xticks(ticks)\n",
    "ax.set_yticks(ticks)\n",
    "ax.set_zticks(ticks)\n",
    "\n",
    "scatter = ax.scatter(data1, data2, data3, c=probability_density_flat, cmap='viridis', alpha = opacity)\n",
    "cbar = fig.colorbar(scatter, ticks=[0, 0.07, 0.14])\n",
    "cbar.ax.tick_params(labelsize=15)\n",
    "plt.show()\n",
    "\n",
    "# Projection onto 2D contour plot\n",
    "fig2 = plt.figure(figsize=(6, 6))\n",
    "ax2 = fig2.add_subplot(111)\n",
    "contour = ax2.contourf(X[:, :, 0], Y[:, :, 0], psi_3d.sum(axis=2), cmap='viridis')\n",
    "plt.yticks([-5, 0, 5], fontsize=23)\n",
    "plt.xticks([-5, 0, 5], fontsize=23)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Energy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(np.array(E_history)[4000:, 0])\n",
    "plt.plot(np.array(E_history)[4000:, 1])\n",
    "plt.legend((\"Model Variable Energy\", \"Variational Method Energy\"))\n",
    "#plt.title(\"Energy Evolution (1D)\")\n",
    "#plt.xlabel(\"Epoch\")\n",
    "#plt.ylabel(\"Energy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(loss_history[:])\n",
    "plt.title(\"Total Loss Across Epochs\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Total Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_experiments = 3\n",
    "num_samples_experiments = [20, 40, 60]\n",
    "model = NN(D, Nh)\n",
    "model_params = model.state_dict()\n",
    "for i in num_samples_experiments:\n",
    "    \n",
    "\n",
    "    learning_rates = {'lambda_layer': 0.1, 'layer1': 0.0005, 'layer2': 0.0005, 'layer3': 0.0005, 'layer4': 0.0005}\n",
    "    parameters = [{'params': getattr(model, name).parameters(), 'lr': lr} for name, lr in learning_rates.items()]\n",
    "\n",
    "    optimizer = optim.Adam(parameters, lr=0.01)\n",
    "    step_size = 1000 \n",
    "    gamma = 0.95\n",
    "    scheduler = lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)\n",
    "\n",
    "    loss_history, E_history, model_list, l_model = train2D(model, optimizer, scheduler, learning_rates, 200000, i, data_range, model_params)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
